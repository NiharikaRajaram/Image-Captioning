# -*- coding: utf-8 -*-
"""ml-final-project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WiwcFxHT8v51hwaUPA39VV0Qm-eJy77h
"""

#Training file
from google.colab import drive
drive.mount('/content/drive')

import string

import numpy as np
from PIL import Image
import os
from pickle import dump, load
import numpy as np
import string
from keras.applications.xception import Xception, preprocess_input
from keras.preprocessing.image import load_img, img_to_array
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical

from keras.layers.merge import add
from keras.models import Model, load_model
from keras.layers import Input, Dense, LSTM, Embedding, Dropout

from tqdm import tqdm_notebook as tqdm
tqdm().pandas()

class Vocabulary:
    def __init__(self, text_data="/content/drive/MyDrive/ML/Flickr8k_text"):
        self.fname = text_data + "/" + "Flickr8k.tkn.txt"

    def capture_all_cptns(self):
        fl = self.document_loader()
        cptns = fl.split('\n')
        desc = {}
        for cap in cptns[:-1]:
            pic, cap = cap.split('\t')
            if pic[:-2] not in desc:
                desc[pic[:-2]] = [cap]
            else:
                desc[pic[:-2]].append(cap)
        return desc

    def document_loader(self):
        fl = open(self.fname, 'r')
        t_cont = fl.read()
        fl.close()
        return t_cont

    def lexicon(self, desc):
        lex = set()

        for key in desc.keys():
            for des in desc[key]:
                lex.update(des.split())

        return lex

    def store_desc(self, desc, description_file):
        list_content = list()
        for key, lst in desc.items():
            for description in lst:
                list_content.append(key + '\t' + description)
        final_dt = "\n".join(list_content)
        fl = open(description_file, "w")
        fl.write(final_dt)
        fl.close()

    def txt_cleaner(self, cptns):
        txt_tbl = str.maketrans('', '', string.punctuation)
        for image, cpns in cptns.items():
            for j, img_cap in enumerate(cpns):
                img_cap.replace("-", " ")
                temp = img_cap.split()
                arr = []
                for wrd in temp:
                    wrd = wrd.lower()
                    # print("lower ",wrd)
                    wrd = wrd.translate(txt_tbl)
                    # print("translated :", wrd)

                    if (len(wrd) > 1) and wrd.isalpha():
                        # print(wrd)
                        arr.append(wrd)


                img_cap = ' '.join(arr)
                cptns[image][j] = img_cap
        return cptns
    arr={}
    def generate_desc_file(self):
        arr = self.capture_all_cptns()
        print("Number of arr =", len(arr))
        print(type(arr))

        cln_desc = self.txt_cleaner(arr)

        lex = self.lexicon(cln_desc)

        self.store_desc(cln_desc, "/content/drive/MyDrive/ML/desc_new2.txt")

        return arr

discriptions_of_all_images=Vocabulary().generate_desc_file()

class Extract_Attributes:
    def __init__(self, data_imgs="/content/drive/MyDrive/ML/Flicker8k_Dataset"):
        self.dir = data_imgs
    
    def extract_attributes(self):
        mod = Xception( include_top=False, pooling='avg' )
        attributes = {}
        for pic in tqdm(os.listdir(self.dir)):
            fname = self.dir + "/" + pic
            img_temp = Image.open(fname)
            img_temp = img_temp.resize((300,300))
            img_temp = np.expand_dims(img_temp, axis=0)
            img_temp = img_temp/127.5
            img_temp = img_temp - 1.0

            attr = mod.predict(img_temp)
            attributes[pic] = attr
        return attributes

    def generate_attributes(self):
        attributes = self.extract_attributes()
        dump(attributes, open("/content/drive/MyDrive/ML/attr_new2.p","wb"))
        attributes = load(open("/content/drive/MyDrive/ML/attr_new2.p","rb"))
        return attributes

attributes=Extract_Attributes().generate_attributes()

def document_loader(fname):
        fl = open(fname, 'r')
        t_cont = fl.read()
        fl.close()
        return t_cont

class Load_Train_Data:
    def __init__(self, text_data="/content/drive/MyDrive/ML/Flickr8k_text"):
        self.fname = text_data + "/" + "Flickr8k.tkn.txt"

    def pictures_load(fname):
        f1 = document_loader(fname)
        pictutes = f1.split("\n")[:-1]
        return pictutes
    
    def desc_loader(fname, pictutes): 
        f2 = document_loader(fname)
        descts = {}
        for val in f2.split("\n"):

            w = val.split()
            if len(w)<1 :
                continue

            img, img_capt = w[0], w[1:]

            if img in pictutes:
                if img not in descts:
                    descts[img] = []
                d = '<start> ' + " ".join(img_capt) + ' <end>'
                descts[img].append(d)

        return descts

    def attributes_load(pictutes):
        attr = load(open("/content/drive/MyDrive/ML/attr_new2.p","rb"))
        attrs = {j:attr[j] for j in pictutes}
        return attrs


    text_data = "/content/drive/MyDrive/ML/Flickr8k_text"

    fname = text_data + "/" + "Flickr_8k.trainImages.txt"

    images_set = pictures_load(fname)
    descriptions_set = desc_loader("/content/drive/MyDrive/ML/desc_new2.txt", images_set)
    attributes_set = attributes_load(images_set)

print(len(Load_Train_Data.descriptions_set))
print(len(Load_Train_Data.images_set))
train_desc = Load_Train_Data.descriptions_set
attributes_trn=Load_Train_Data.attributes_set
print(train_desc)
# print(Load_Train_Data.attributes_set)

def clean_dict(descts):
    diccriptions_arr = []
    for key in descts.keys():
        [diccriptions_arr.append(d) for d in descts[key]]
    return diccriptions_arr

class Gen_Tokenizer:
	def generate_tokens(details):
		lst = clean_dict(details)
		tknzr = Tokenizer()
		tknzr.fit_on_texts(lst)
		return tknzr
	
tokens_generated = Gen_Tokenizer.generate_tokens(Load_Train_Data.descriptions_set)
print(tokens_generated)
dump(tokens_generated, open('/content/drive/MyDrive/ML/tokenizer_new2.p', 'wb'))
tot_vocabulary = len(tokens_generated.word_index) + 1
print(tot_vocabulary)

def len_maximum(descts):
    description_lst = clean_dict(descts)
    return max(len(m.split()) for m in description_lst)
    
len_maximum = len_maximum(discriptions_of_all_images)
len_maximum

class Data_Generator:
    def data_generator(desc, attributes, tokens_generated, len_maximum):
        while 1:
            for key, lst in desc.items():
                attribute = attributes[key][0]
                img_input, sequence_input, output = sequence_generator(tokens_generated, len_maximum, lst, attribute)
                yield ([img_input, sequence_input], output)

def sequence_generator(tokens_generated, len_maximum, lis_dscrptns, attribute):
	attr1, attr2, attr_y1 = list(), list(), list()
	for des in lis_dscrptns:
		sqnce = tokens_generated.texts_to_sequences([des])[0]
		for i in range(1, len(sqnce)):
			input_sqnce, output_sqnce = sqnce[:i], sqnce[i]
			input_sqnce = pad_sequences([input_sqnce], maxlen=len_maximum)[0]
			output_sqnce = to_categorical([output_sqnce], num_classes=tot_vocabulary)[0]
			attr1.append(attribute)
			attr2.append(input_sqnce)
			attr_y1.append(output_sqnce)
	return np.array(attr1), np.array(attr2), np.array(attr_y1)

[a1,a2],a3 = next(Data_Generator.data_generator(train_desc, attributes, tokens_generated, len_maximum))
a1.shape, a2.shape, a3.shape

from tensorflow.keras.utils import plot_model

class Mod_Generator:
  def mod_definition(tot_vocabulary,maximum_length):
    ip1 = Input(shape=(2048,))
    at1 = Dropout(0.5)(ip1)
    at2 = Dense(256, activation='relu')(at1)
    
    ip2 = Input(shape=(maximum_length,))
    se_one = Embedding(tot_vocabulary, 256, mask_zero=True)(ip2)
    se_two = Dropout(0.5)(se_one)
    se_three = LSTM(256)(se_two)
    
    dcodr1 = add([at2, se_three])
    dcodr2 = Dense(256, activation='relu')(dcodr1)
    op = Dense(tot_vocabulary, activation='softmax')(dcodr2)
    print(op)

    mod = Model( inputs = [ip1, ip2], outputs = op )
    mod.compile( loss = 'categorical_crossentropy' , optimizer = 'adam' )
    print(mod.summary())
    plot_model(mod, to_file='/content/drive/MyDrive/ML/gen_mod.png', show_shapes=True)
    return mod
	
gen_mod=Mod_Generator.mod_definition(tot_vocabulary,len_maximum)

iterations=10
len_train_desc=len(train_desc)
for j in range(iterations):
  gen=Data_Generator.data_generator(train_desc,attributes_trn,tokens_generated,len_maximum)
  gen_mod.fit_generator(gen,epochs=1,steps_per_epoch=len_train_desc,verbose=1)
  gen_mod.save("/content/drive/MyDrive/ML/models/final_mod" + str(j) + ".h5")


# Testing file

ap = argparse.ArgumentParser()
ap.add_argument('-i', '--image', required=True, help="Image Path")
args = vars(ap.parse_args())
img_path = args['image']
# img_path = '/content/drive/MyDrive/ML/Flicker8k_Dataset/111537222_07e56d5a30.jpg'

def extract_attributes(f1, mdl):
        try:
            picture = Image.open(f1)
            
        except:
            print("ERROR: Couldn't open image! Make sure the image path and extension is correct")
        picture = picture.resize((299,299))
        picture = np.array(picture)
        # for images that has 4 channels, we convert them into 3 channels
        if picture.shape[2] == 4: 
            picture = picture[..., :3]
        picture = np.expand_dims(picture, axis=0)
        picture = picture/127.5
        picture = picture - 1.0
        attr = mdl.predict(picture)
        return attr

def wrd_for_id(integer, gen_tokenizer):
  for wrd, idx in gen_tokenizer.word_index.items():
      if idx == integer:
          return wrd
  return None


def generate_desc(model, tokens_generated, img, len_maximum):
    in_txt = 'start'
    for i in range(len_maximum):
        seqnc = tokens_generated.texts_to_sequences([in_txt])[0]
        seqnc = pad_sequences([seqnc], maxlen=len_maximum)
        pred = model.predict([img,seqnc], verbose=0)
        pred = np.argmax(pred)
        wrd = wrd_for_id(pred, tokens_generated)
        if wrd is None:
            break
        in_txt += ' ' + wrd
        if wrd == 'end':
            break
    return in_txt

def load_document(filename):
        # Opening the file as read only
        file = open(filename, 'r')
        text_content = file.read()
        file.close()
        return text_content
    
    
def get_saved_description(filename, photo):
    file = load_document(filename)
    descriptions = {}
    for line in file.split("\n"):
        words = line.split()
        if len(words) < 1 :
            continue

        image, image_caption = words[0], words[1:]
        
        if image not in descriptions:
            descriptions[image] = []
            
        descriptions[image].append(image_caption)

    return descriptions[image]

filename = "/content/drive/MyDrive/ML/descriptions.txt"

len_maximum = 32
tokens_generated = load(open("/content/drive/MyDrive/ML/tokenizer_new2.p","rb"))
loaded_model = load_model('/content/drive/MyDrive/ML/models/final_mod9.h5')
x_import_model = Xception(include_top=False, pooling="avg")



picture = extract_attributes(img_path, x_import_model)
img_new = Image.open(img_path)

description_tokens = generate_desc(loaded_model, tokens_generated, picture, len_maximum)
print(description_tokens)
pred = []
pred.append(description_tokens.split())
print("predicted ", pred)

from nltk.translate.bleu_score import corpus_bleu # BLEU Score

actual = get_saved_description(filename, picture)
references = []
references.append(actual)
print("actual", actual)
print("references", references)

print('BLEU-1: %f' % corpus_bleu(references, pred, weights=(0,0,0.5,0.5)))
print('BLEU-2: %f' % corpus_bleu(references, pred, weights=(0, 0, 0.25, 0.75)))
print("\n\n")
print(description_tokens)
plt.imshow(img_new)